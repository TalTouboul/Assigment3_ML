{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - Text Analysis\n",
    "An explanation this assignment could be found in the .pdf explanation document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Materials to review for this assignment\n",
    "<h4>From Moodle:</h4> \n",
    "<h5><u>Review the notebooks regarding the following python topics</u>:</h5>\n",
    "<div class=\"alert alert-info\">\n",
    "&#x2714; <b>Working with strings</b> (tutorial notebook)<br/>\n",
    "&#x2714; <b>Text Analysis</b> (tutorial notebook)<br/>\n",
    "&#x2714; <b>Hebrew text analysis tools (tokenizer, wordnet)</b> (moodle example)<br/>\n",
    "&#x2714; <b>(brief review) All previous notebooks</b><br/>\n",
    "</div> \n",
    "<h5><u>Review the presentations regarding the following topics</u>:</h5>\n",
    "<div class=\"alert alert-info\">\n",
    "&#x2714; <b>Text Analysis</b> (lecture presentation)<br/>\n",
    "&#x2714; <b>(brief review) All other presentations</b><br/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personal Details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Details Student 1:\n",
    "# Name : Tal Tubul\n",
    "# I.D : 208835355\n",
    "# Email : taltub123@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preceding Step - import modules (packages)\n",
    "This step is necessary in order to use external modules (packages). <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# --------------------------------------\n",
    "\n",
    "\n",
    "# --------------------------------------\n",
    "# ------------- visualizations:\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "# --------------------------------------\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "import sklearn\n",
    "from sklearn import preprocessing, metrics, pipeline, model_selection, feature_extraction \n",
    "from sklearn import naive_bayes, linear_model, svm, neural_network, neighbors, tree\n",
    "from sklearn import decomposition, cluster\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score, silhouette_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import Perceptron, SGDClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# ---------------------------------------\n",
    "\n",
    "\n",
    "# ----------------- output and visualizations: \n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.simplefilter(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "# show several prints in one cell. This will allow us to condence every trick in one cell.\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline\n",
    "pd.pandas.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "# ---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text analysis and String manipulation imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# --------- Text analysis and Hebrew text analysis imports:\n",
    "# vectorizers:\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# regular expressions:\n",
    "import re\n",
    "# --------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) Hebrew text analysis - WordNet (for Hebrew)\n",
    "Note: the WordNet is not a must"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (optional) Only if you didn't install Wordnet (for Hebrew) use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word net installation:\n",
    "\n",
    "# unmark if you want to use and need to install\n",
    "# !pip install wn\n",
    "# !python -m wn download omw-he:1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word net import:\n",
    "\n",
    "# unmark if you want to use:\n",
    "import wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) Hebrew text analysis - hebrew_tokenizer (Tokenizer for Hebrew)\n",
    "Note: the hebrew_tokenizer is not a must"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (optional) Only if you didn't install hebrew_tokenizer use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hebrew tokenizer installation:\n",
    "\n",
    "# unmark if you want to use and need to install:\n",
    "# !pip install hebrew_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tal\\Desktop\\לימודים\\למידת מכונה\\מטלה 3\n"
     ]
    }
   ],
   "source": [
    "# Hebrew tokenizer import:\n",
    "\n",
    "# unmark if you want to use:\n",
    "import hebrew_tokenizer as ht"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading input files\n",
    "Reading input files for train annotated corpus (raw text data) corpus and for the test corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = 'annotated_corpus_for_train.csv'\n",
    "test_filename  = 'corpus_for_test.csv'\n",
    "df_train = pd.read_csv(train_filename, index_col=None, encoding='utf-8')\n",
    "df_test  = pd.read_csv(test_filename, index_col=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>כשחבר הזמין אותי לחול, לא באמת חשבתי שזה יקרה,...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>לפני שהתגייסתי לצבא עשיתי כל מני מיונים ליחידו...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>מאז שהתחילו הלימודים חלומו של כל סטודנט זה הפנ...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>כשהייתי ילד, מטוסים היה הדבר שהכי ריתק אותי. ב...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>‏הייתי מדריכה בכפר נוער ומתאם הכפר היינו צריכי...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>לפני כ3 חודשים טסתי לרומא למשך שבוע. טסתי במטו...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>אני כבר שנתיים נשוי והשנה אני ואישתי סוף סוף י...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>השנה התחלנו שיפוץ בדירה שלנו בתל אביב. הדירה ה...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               story gender\n",
       "0  כשחבר הזמין אותי לחול, לא באמת חשבתי שזה יקרה,...      m\n",
       "1  לפני שהתגייסתי לצבא עשיתי כל מני מיונים ליחידו...      m\n",
       "2  מאז שהתחילו הלימודים חלומו של כל סטודנט זה הפנ...      f\n",
       "3  כשהייתי ילד, מטוסים היה הדבר שהכי ריתק אותי. ב...      m\n",
       "4  ‏הייתי מדריכה בכפר נוער ומתאם הכפר היינו צריכי...      f\n",
       "5  לפני כ3 חודשים טסתי לרומא למשך שבוע. טסתי במטו...      f\n",
       "6  אני כבר שנתיים נשוי והשנה אני ואישתי סוף סוף י...      m\n",
       "7  השנה התחלנו שיפוץ בדירה שלנו בתל אביב. הדירה ה...      f"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(753, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(8)\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_example_id</th>\n",
       "      <th>story</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>כל קיץ אני והמשפחה נוסעים לארצות הברית לוס אנג...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>הגעתי לשירות המדינה אחרי שנתיים כפעיל בתנועת \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>אחת האהבות הגדולות שלי אלו הכלבים שלי ושל אישת...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_example_id                                              story\n",
       "0                0  כל קיץ אני והמשפחה נוסעים לארצות הברית לוס אנג...\n",
       "1                1  הגעתי לשירות המדינה אחרי שנתיים כפעיל בתנועת \"...\n",
       "2                2  אחת האהבות הגדולות שלי אלו הכלבים שלי ושל אישת..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(323, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(3)\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your implementation:\n",
    "Write your code solution in the following code-cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function: translate_df_to_english(df)\n",
    "Translates the content in the 'story' column of a given DataFrame from Hebrew to English characters, creating a new DataFrame with translated text. Useful for converting non-English text to English for analysis.\n",
    "\n",
    "**Input:** \n",
    "df (DataFrame): DataFrame with a 'story' column containing Hebrew text.\n",
    "\n",
    "**Output:** \n",
    "Returns a new DataFrame with 'story' column content translated to English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_df_to_english(df):\n",
    "    \n",
    "    english_df = df.copy()\n",
    "    translation_table = str.maketrans(\n",
    "        \"אבגדהוזחטיכלמנסעפצקרשתםףץןך\",\n",
    "        \"abgdhwzxviklmnsypcqretMFCNX\"\n",
    "    )\n",
    "    for row_index in range(0,len(english_df)):\n",
    "        \n",
    "        english_df.loc[row_index, 'story'] = english_df.loc[row_index, 'story'].translate(translation_table)\n",
    "\n",
    "    return english_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function: create_count_vectorizer(ngram_range_index)\n",
    "Creates a CountVectorizer object with a specified ngram range.\n",
    "\n",
    "**Input:** \n",
    "ngram_range_index: An integer indicating the desired ngram range.\n",
    "\n",
    "**Output:** \n",
    "A CountVectorizer object configured with the specified ngram range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_count_vectorizer(ngram_range_index):\n",
    "    return CountVectorizer(ngram_range=(ngram_range_index, ngram_range_index)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function: create_Tfidf_Vectorizer(ngram_range_index)\n",
    "Creates a TfidfVectorizer object with a specified ngram range.\n",
    "\n",
    "**Input:** \n",
    "ngram_range_index: An integer indicating the desired ngram range.\n",
    "\n",
    "**Output:** \n",
    "A TfidfVectorizer object configured with the specified ngram range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Tfidf_Vectorizer(ngram_range_index):\n",
    "    return TfidfVectorizer(ngram_range=(ngram_range_index, ngram_range_index)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function: normailze(X_train, X_test)\n",
    "Takes the training and test data matrices as input, normalizes them using the StandardScaler, and returns the normalized versions of the matrices.\n",
    "\n",
    "**Input:** \n",
    "1. X_train: Training data matrix (sparse or dense).\n",
    "2. X_test: Test data matrix (sparse or dense).\n",
    "\n",
    "**Output:** \n",
    "1. X_train_normalized: Normalized training data matrix.\n",
    "2. X_test_normalized: Normalized test data matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normailze(X_train, X_test):\n",
    "    \n",
    "    scalar = StandardScaler()\n",
    "\n",
    "    X_train_normalized = scalar.fit_transform(X_train.toarray())\n",
    "    X_test_normalized = scalar.transform(X_test.toarray())\n",
    "\n",
    "    return X_train_normalized, X_test_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function: fit(clf, X_train_normalized, y_train)\n",
    " Fits a given classifier to the normalized training data and their corresponding target labels. It's a convenient wrapper to call the fit method of the classifier.\n",
    "\n",
    "**Input:** \n",
    "1. clf: The classifier model.\n",
    "2. X_train_normalized: Normalized training data.\n",
    "3. y_train: Target label.\n",
    "\n",
    "**Output:** \n",
    "non output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(clf, X_train_normalized, y_train):\n",
    "\n",
    "    clf.fit(X_train_normalized, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function: predict(clf, X_test_normalized)\n",
    "Takes the training and test data matrices as input, normalizes them using the StandardScaler, and returns the normalized versions of the matrices.\n",
    "\n",
    "**Input:** \n",
    "1. clf: The trained classifier model.\n",
    "2. X_test_normalized: Normalized test data.\n",
    "\n",
    "**Output:** \n",
    "The function returns an array of predicted labels for the given test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(clf, X_test_normalized):\n",
    "\n",
    "    return clf.predict(X_test_normalized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function: evaluate_accuracy(y_test, y_predicted)\n",
    "Calculates the accuracy of a classification model by comparing the predicted labels with the actual labels of the test data.\n",
    "\n",
    "**Input:** \n",
    "1. y_test: The true labels of the test data.\n",
    "2. y_predicted: The predicted labels of the test data.\n",
    "\n",
    "**Output:** \n",
    "The function returns the accuracy of the model, which is the proportion of correctly predicted labels among all the test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(y_test, y_predicted):\n",
    "\n",
    "    return  np.mean(y_predicted == y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function: f1_score_calc(testDf, y_pred)\n",
    "Calculates the average F1 score.\n",
    "\n",
    "**Input:** \n",
    "1. testDf: A DataFrame containing the test examples and their true gender labels.\n",
    "2. y_pred: The predicted gender labels of the test examples.\n",
    "\n",
    "**Output:** \n",
    "The function returns the average F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_calc(testDf, y_pred):\n",
    "    f1_male = f1_score(testDf.gender, y_pred, pos_label='m')\n",
    "    f1_female = f1_score(testDf.gender, y_pred, pos_label='f')\n",
    "    average_f1 = (f1_male + f1_female) / 2\n",
    "\n",
    "    return average_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function: create_Perceptron_model(X_train_normalized, X_test_normalized, y_train, y_test, testDf)\n",
    " Creates and evaluates a Perceptron model.\n",
    "\n",
    "**Input:** \n",
    "1. X_train_normalized: Normalized training data.\n",
    "2. X_test_normalized: Normalized test data (features).\n",
    "3. y_train: True gender labels.\n",
    "4. y_test: True gender labels.\n",
    "5. testDf: DataFrame containing the test examples and their true gender labels.\n",
    "\n",
    "**Output:** \n",
    "1. Average F1 score.\n",
    "2. Accuracy of the Perceptron model.\n",
    "3. The best alpha index for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Perceptron_model(X_train_normalized, X_test_normalized, y_train, y_test, testDf):\n",
    "    max_alpha = {\n",
    "        \"average_f1_perceptron_model\" : 0,\n",
    "        \"perceptron_model_accuracy\" : 0,\n",
    "        \"alpha_index\" : \"\"\n",
    "    }\n",
    "    for alpha_index in [0.0001, 0.005, 0.01]:\n",
    "        perceptron_model = Perceptron(alpha=alpha_index, max_iter=20)\n",
    "        fit(perceptron_model, X_train_normalized, y_train)\n",
    "        y_pred_perceptron_model = predict(perceptron_model, X_test_normalized)\n",
    "        perceptron_model_accuracy = evaluate_accuracy(y_test, y_pred_perceptron_model)\n",
    "        average_f1_perceptron_model = f1_score_calc(testDf, y_pred_perceptron_model)\n",
    "        if average_f1_perceptron_model > max_alpha[\"average_f1_perceptron_model\"]:\n",
    "            max_alpha[\"average_f1_perceptron_model\"] = average_f1_perceptron_model\n",
    "            max_alpha[\"perceptron_model_accuracy\"] = perceptron_model_accuracy\n",
    "            max_alpha[\"alpha_index\"] = alpha_index\n",
    "    return max_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function: create_LinearSVC_model(X_train_normalized, X_test_normalized, y_train, y_test, testDf)\n",
    " Creates and evaluates a Linear Support Vector Classification (LinearSVC) model.\n",
    "\n",
    "**Input:** \n",
    "1. X_train_normalized: Normalized training data.\n",
    "2. X_test_normalized: Normalized test data (features).\n",
    "3. y_train: True gender labels.\n",
    "4. y_test: True gender labels.\n",
    "5. testDf: DataFrame containing the test examples and their true gender labels.\n",
    "\n",
    "**Output:** \n",
    "1. Average F1 score.\n",
    "2. Accuracy of the Perceptron model.\n",
    "3. The best C index for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_LinearSVC_model(X_train_normalized, X_test_normalized, y_train, y_test, testDf):\n",
    "    max_C = {\n",
    "        \"average_f1_LinearSVC_model\" : 0,\n",
    "        \"LinearSVC_model_accuracy\" : 0,\n",
    "        \"C_index\" : \"\"\n",
    "    }\n",
    "    for C_index in [0.0001, 0.005, 0.01]:\n",
    "        LinearSVC_model = LinearSVC(C= 3, max_iter= 20)\n",
    "        fit(LinearSVC_model, X_train_normalized, y_train)\n",
    "        y_pred_LinearSVC_model = predict(LinearSVC_model, X_test_normalized)\n",
    "        LinearSVC_model_accuracy = evaluate_accuracy(y_test, y_pred_LinearSVC_model)\n",
    "        average_f1_LinearSVC_model = f1_score_calc(testDf, y_pred_LinearSVC_model)\n",
    "        if average_f1_LinearSVC_model > max_C[\"average_f1_LinearSVC_model\"]:\n",
    "            max_C[\"average_f1_LinearSVC_model\"] = average_f1_LinearSVC_model\n",
    "            max_C[\"LinearSVC_model_accuracy\"] = LinearSVC_model_accuracy\n",
    "            max_C[\"C_index\"] = C_index\n",
    "    return max_C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function: create_SGDClassifier_model(X_train_normalized, X_test_normalized, y_train, y_test, testDf)\n",
    " Creates and evaluates a Stochastic Gradient Descent (SGD) Classifier model.\n",
    "\n",
    "**Input:** \n",
    "1. X_train_normalized: Normalized training data.\n",
    "2. X_test_normalized: Normalized test data (features).\n",
    "3. y_train: True gender labels.\n",
    "4. y_test: True gender labels.\n",
    "5. testDf: DataFrame containing the test examples and their true gender labels.\n",
    "\n",
    "**Output:** \n",
    "1. Average F1 score.\n",
    "2. Accuracy of the Perceptron model.\n",
    "3. The best loss index for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_SGDClassifier_model(X_train_normalized, X_test_normalized, y_train, y_test, testDf):\n",
    "    max_loss = {\n",
    "        \"average_f1_SGDClassifier_model\" : 0,\n",
    "        \"SGDClassifier_model_accuracy\" : 0,\n",
    "        \"loss_index\" : \"\"\n",
    "    }\n",
    "    for loss_index in ['hinge', 'log_loss', 'log', 'modified_huber', 'squared_hinge', 'perceptron', 'squared_error', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive']:\n",
    "        SGDClassifier_model = SGDClassifier(loss=loss_index, penalty='l1', alpha=0.001, random_state=42, max_iter=10, tol= None)\n",
    "        fit(SGDClassifier_model, X_train_normalized, y_train)\n",
    "        y_pred_SGDClassifier_model = predict(SGDClassifier_model, X_test_normalized)\n",
    "        SGDClassifier_model_accuracy = evaluate_accuracy(y_test, y_pred_SGDClassifier_model)\n",
    "        average_f1_SGDClassifier_model = f1_score_calc(testDf, y_pred_SGDClassifier_model)\n",
    "        if average_f1_SGDClassifier_model > max_loss[\"average_f1_SGDClassifier_model\"]:\n",
    "            max_loss[\"average_f1_SGDClassifier_model\"] = average_f1_SGDClassifier_model\n",
    "            max_loss[\"SGDClassifier_model_accuracy\"] = SGDClassifier_model_accuracy\n",
    "            max_loss[\"loss_index\"] = loss_index\n",
    "    return max_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function: split_to_trainDF_testDF()\n",
    "splits the provided DataFrame df_train into training and test DataFrames. \n",
    "\n",
    "**Note:** I splitted the train_df to perform experiments and test different models before I run the model on the test_df\n",
    "\n",
    "**Input:** \n",
    "None \n",
    "\n",
    "**Output:** \n",
    "1. trainDf: DataFrame containing training examples.\n",
    "2. testDf: DataFrame containing test examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_trainDF_testDF():\n",
    "    \n",
    "    english_df_train = translate_df_to_english(df_train)\n",
    "\n",
    "    indexed_df_train = english_df_train.copy()\n",
    "    indexed_df_train[\"id\"] = indexed_df_train.index\n",
    "    \n",
    "    trainDf=indexed_df_train[indexed_df_train[\"id\"]%5!=0]\n",
    "    testDf=indexed_df_train[indexed_df_train[\"id\"]%5==0]\n",
    "\n",
    "    return trainDf, testDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function: max_f1_score_for_vectorizer(vectorizer, vectorizer_type ,trainDf, testDf, y_train, y_test, max_average_f1_dict, ngram_range_index)\n",
    "Calculates the maximum F1 score achieved among different models (Perceptron, LinearSVC, SGDClassifier) for a given vectorizer type and n-gram range index.\n",
    "\n",
    "The function performs the following steps:\n",
    "\n",
    "* Transforms text data using the provided vectorizer.\n",
    "* Normalizes the transformed data.\n",
    "* Calls the functions to create models for Perceptron, LinearSVC, and SGDClassifier, calculates their F1 scores and accuracy.\n",
    "* Compares the F1 scores of these models and updates the max_average_f1_dict if a higher F1 score is found.\n",
    "* Returns the updated max_average_f1_dict.\n",
    "\n",
    "**Input:** \n",
    "1. vectorizer: Vectorizer object for feature extraction.\n",
    "2. vectorizer_type: Type of the vectorizer used (for updating the dictionary).\n",
    "3. trainDf: DataFrame containing training examples.\n",
    "4. testDf: DataFrame containing test examples.\n",
    "5. y_train: Labels for training examples.\n",
    "6. y_test: Labels for test examples.\n",
    "7. max_average_f1_dict: Dictionary to store maximum F1 score information.\n",
    "8. ngram_range_index: Index for n-gram range.\n",
    "\n",
    "**Output:** \n",
    "1. max_average_f1_dict: Updated dictionary with maximum F1 score information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_f1_score_for_vectorizer(vectorizer, vectorizer_type ,trainDf, testDf, y_train, y_test, max_average_f1_dict, ngram_range_index):\n",
    "\n",
    "    X_train = vectorizer.fit_transform(trainDf['story'])\n",
    "    X_test = vectorizer.transform(testDf['story'])\n",
    "\n",
    "    X_train_normalized, X_test_normalized = normailze(X_train, X_test)\n",
    "\n",
    "    perceptron_model_dict = create_Perceptron_model(X_train_normalized, X_test_normalized, y_train, y_test, testDf)\n",
    "    LinearSVC_model_dict = create_LinearSVC_model(X_train_normalized, X_test_normalized, y_train, y_test, testDf)\n",
    "    SGDClassifier_model_dict = create_SGDClassifier_model(X_train_normalized, X_test_normalized, y_train, y_test, testDf)  \n",
    "    \n",
    "    max_f1_score =  max(perceptron_model_dict[\"average_f1_perceptron_model\"], LinearSVC_model_dict[\"average_f1_LinearSVC_model\"], SGDClassifier_model_dict[\"average_f1_SGDClassifier_model\"])\n",
    "\n",
    "    if max_f1_score > max_average_f1_dict['average_f1_score']:\n",
    "        max_average_f1_dict['vectorizer_type'] = vectorizer_type\n",
    "        max_average_f1_dict['ngram_range_index'] = ngram_range_index\n",
    "        \n",
    "        if max_f1_score == perceptron_model_dict[\"average_f1_perceptron_model\"]:\n",
    "            max_average_f1_dict['model_type'] = 'Perceptron'\n",
    "            max_average_f1_dict['average_f1_score'] = perceptron_model_dict[\"average_f1_perceptron_model\"]\n",
    "            max_average_f1_dict['evaluate_accuracy'] = perceptron_model_dict[\"perceptron_model_accuracy\"]\n",
    "            max_average_f1_dict['hyper_parameter'] = perceptron_model_dict[\"alpha_index\"]\n",
    "\n",
    "        elif max_f1_score == LinearSVC_model_dict[\"average_f1_LinearSVC_model\"]:\n",
    "            max_average_f1_dict['model_type'] = 'LinearSVC'\n",
    "            max_average_f1_dict['average_f1_score'] = LinearSVC_model_dict[\"average_f1_LinearSVC_model\"]\n",
    "            max_average_f1_dict['evaluate_accuracy'] = LinearSVC_model_dict[\"LinearSVC_model_accuracy\"]\n",
    "            max_average_f1_dict['hyper_parameter'] = LinearSVC_model_dict[\"C_index\"]\n",
    "\n",
    "        else:\n",
    "            max_average_f1_dict['model_type'] = 'SGDClassifier'\n",
    "            max_average_f1_dict['average_f1_score'] = SGDClassifier_model_dict[\"average_f1_SGDClassifier_model\"]\n",
    "            max_average_f1_dict['evaluate_accuracy'] = SGDClassifier_model_dict[\"SGDClassifier_model_accuracy\"]\n",
    "            max_average_f1_dict['hyper_parameter'] = SGDClassifier_model_dict[\"loss_index\"]\n",
    "    \n",
    "    return max_average_f1_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function: split_to_trainDF_testDF()\n",
    "This function finds the best performing model among different combinations of n-gram ranges and vectorizer types. The function follows these steps:\n",
    "* Splits the data into training and testing sets using split_to_trainDF_testDF() function.\n",
    "* Defines labels for training and testing sets.\n",
    "* Initializes a dictionary max_average_f1_dict to store information about the best model.\n",
    "* Iterates through different n-gram ranges (1 to 3) and for each range:\n",
    "    1. Creates a CountVectorizer and a TfidfVectorizer.\n",
    "    2. Calls the max_f1_score_for_vectorizer() function to determine the best model type and vectorizer type for the current n-gram range.\n",
    "    3. Updates the max_average_f1_dict with the best performing model's information.\n",
    "* Returns the max_average_f1_dict containing information about the best performing model.\n",
    "\n",
    "**Input:** \n",
    "None \n",
    "\n",
    "**Output:** \n",
    "1. max_average_f1_dict: A dictionary containing information about the best performing model, including n-gram range, vectorizer type, model type, average F1 score, and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_model():\n",
    "\n",
    "    trainDf, testDf = split_to_trainDF_testDF()\n",
    "\n",
    "    y_train = trainDf['gender']\n",
    "    y_test = testDf['gender']\n",
    "\n",
    "\n",
    "    max_average_f1_dict = {\n",
    "        'ngram_range_index' : 0,\n",
    "        'vectorizer_type' : '',\n",
    "        'model_type' : '',\n",
    "        'average_f1_score' : 0,\n",
    "        'evaluate_accuracy' : 0,\n",
    "        'hyper_parameter' : 0\n",
    "    }\n",
    "\n",
    "    for ngram_range_index in range(1,4):\n",
    "        count_vectorizer = create_count_vectorizer(ngram_range_index)\n",
    "        max_average_f1_dict = max_f1_score_for_vectorizer(count_vectorizer, 'count_vectorizer' ,trainDf, testDf, y_train, y_test, max_average_f1_dict, ngram_range_index)\n",
    "\n",
    "        Tfidf_Vectorizer = create_Tfidf_Vectorizer(ngram_range_index)\n",
    "        max_average_f1_dict = max_f1_score_for_vectorizer(Tfidf_Vectorizer, 'Tfidf_Vectorizer' ,trainDf, testDf, y_train, y_test, max_average_f1_dict, ngram_range_index)\n",
    "\n",
    "    return max_average_f1_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calls to find_best_model() to find the best model to use for the test_df\n",
    "best_model_dict = find_best_model() #BE CEARFUL!!! This function running for 18 minitus if you want to save your time run this command instead :\n",
    "\"\"\" \n",
    "best_model_dict = {\n",
    "        'ngram_range_index' : 2,\n",
    "        'vectorizer_type' : 'count_vectorizer',\n",
    "        'model_type' : 'SGDClassifier',\n",
    "        'average_f1_score' : 0.7276233128071158,\n",
    "        'evaluate_accuracy' : 0.8079470198675497,\n",
    "        'hyper_parameter' : \"hinge\"\n",
    "    }\n",
    "\"\"\"\n",
    "# Translate both training and testing data to English\n",
    "english_df_train = translate_df_to_english(df_train)\n",
    "english_df_test = translate_df_to_english(df_test)\n",
    "\n",
    "# Prepare the target labels for training\n",
    "y_train = english_df_train['gender']\n",
    "\n",
    "# Choose the appropriate vectorizer based on the best model's vectorizer type\n",
    "if best_model_dict['vectorizer_type'] == 'count_vectorizer':\n",
    "    vectorizer = create_count_vectorizer(best_model_dict['ngram_range_index'])\n",
    "else:\n",
    "    vectorizer = create_Tfidf_Vectorizer(best_model_dict['ngram_range_index'])\n",
    "\n",
    "# Transform text data into numerical features\n",
    "X_train = vectorizer.fit_transform(english_df_train['story'])\n",
    "X_test = vectorizer.transform(english_df_test['story'])\n",
    "\n",
    "# Normalize the feature data\n",
    "X_train_normalized, X_test_normalized = normailze(X_train, X_test)\n",
    "\n",
    "# Choose the appropriate model based on the best model's type\n",
    "if best_model_dict['model_type'] == 'Perceptron':\n",
    "    model = Perceptron(alpha=best_model_dict['hyper_parameter'], max_iter=20)\n",
    "elif best_model_dict['model_type'] == 'LinearSVC':\n",
    "    model = LinearSVC(C=best_model_dict['hyper_parameter'], max_iter= 20)\n",
    "else:\n",
    "    model = SGDClassifier(loss=best_model_dict['hyper_parameter'], penalty='l1', alpha=0.001, random_state=42, max_iter=10, tol= None)\n",
    "\n",
    "# Train the chosen model\n",
    "fit(model, X_train_normalized, y_train)\n",
    "\n",
    "# Predict gender using the trained model\n",
    "y_pred_model = predict(model, X_test_normalized)\n",
    "\n",
    "# Select only the 'test_example_id' and 'gender' columns for the final output DataFrame\n",
    "df_test['gender'] = y_pred_model\n",
    "columns = ['test_example_id', 'gender']\n",
    "df_predicted = df_test[columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model that I found is:\n",
      "Vectorizer type: count_vectorizer\n",
      "Ngram range index: 2\n",
      "Model type: SGDClassifier\n",
      "Hyper parameter value: hinge\n",
      "Average f1 score: 0.7276233128071158\n",
      "Evaluate accuracy: 0.8079470198675497\n",
      "\n",
      "The y predicted model results are:\n",
      "Gender 'f' count: 43\n",
      "Gender 'm' count: 280\n",
      "The y_pred_model results: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['m', 'm', 'm', 'f', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm',\n",
       "       'm', 'f', 'm', 'm', 'm', 'f', 'm', 'm', 'm', 'm', 'm', 'm', 'f',\n",
       "       'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'f', 'm', 'm', 'm',\n",
       "       'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'f', 'm',\n",
       "       'm', 'f', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'f',\n",
       "       'm', 'm', 'f', 'm', 'm', 'm', 'm', 'm', 'm', 'f', 'm', 'm', 'm',\n",
       "       'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'f', 'm', 'm', 'm',\n",
       "       'm', 'm', 'm', 'f', 'm', 'm', 'm', 'm', 'f', 'm', 'm', 'm', 'm',\n",
       "       'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'f', 'm', 'm', 'm', 'm',\n",
       "       'm', 'm', 'm', 'm', 'f', 'm', 'm', 'm', 'f', 'm', 'f', 'm', 'm',\n",
       "       'm', 'm', 'm', 'm', 'f', 'f', 'm', 'm', 'f', 'm', 'm', 'm', 'f',\n",
       "       'f', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'f', 'm', 'm', 'm', 'm',\n",
       "       'm', 'm', 'm', 'm', 'm', 'm', 'm', 'f', 'm', 'f', 'm', 'm', 'm',\n",
       "       'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm',\n",
       "       'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm',\n",
       "       'm', 'm', 'm', 'm', 'f', 'f', 'f', 'm', 'f', 'm', 'm', 'm', 'm',\n",
       "       'f', 'm', 'f', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm',\n",
       "       'm', 'm', 'm', 'm', 'm', 'f', 'm', 'm', 'f', 'm', 'm', 'm', 'm',\n",
       "       'm', 'm', 'f', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm',\n",
       "       'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm',\n",
       "       'm', 'm', 'm', 'm', 'm', 'm', 'm', 'f', 'm', 'm', 'm', 'm', 'm',\n",
       "       'm', 'm', 'm', 'm', 'f', 'f', 'm', 'f', 'm', 'm', 'm', 'm', 'm',\n",
       "       'm', 'm', 'f', 'm', 'm', 'f', 'f', 'm', 'm', 'm', 'm', 'm', 'm',\n",
       "       'm', 'm', 'm', 'm', 'm', 'f', 'm', 'm', 'm', 'm', 'm', 'm', 'm',\n",
       "       'm', 'm', 'f', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm'], dtype='<U1')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Display the best model information\n",
    "print(\"The best model that I found is:\")\n",
    "print(f\"Vectorizer type: {best_model_dict['vectorizer_type']}\")\n",
    "print(f\"Ngram range index: {best_model_dict['ngram_range_index']}\")\n",
    "print(f\"Model type: {best_model_dict['model_type']}\")\n",
    "print(f\"Hyper parameter value: {best_model_dict['hyper_parameter']}\")\n",
    "print(f\"Average f1 score: {best_model_dict['average_f1_score']}\")\n",
    "print(f\"Evaluate accuracy: {best_model_dict['evaluate_accuracy']}\\n\")\n",
    "\n",
    "# Calculate and display the predicted model results\n",
    "y_pred_counts = np.unique(y_pred_model, return_counts=True)\n",
    "print(f\"The y predicted model results are:\")\n",
    "print(f\"Gender 'f' count: {y_pred_counts[1][0]}\")\n",
    "print(f\"Gender 'm' count: {y_pred_counts[1][1]}\")\n",
    "print(\"The y_pred_model results: \")\n",
    "y_pred_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the purpose of comparison, I built a model without hyperparameters of the SGDClassifier model type and without normalization to show that **the accuracy of the model has decreased.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we can see that the f1_score of the comparison_model decreased to 0.6177215189873418 instead 0.7276233128071158\n",
      "And the accuracy of the comparison_model decreased to 0.7417218543046358 instead 0.8079470198675497\n"
     ]
    }
   ],
   "source": [
    "#Splits the df_train to train and test\n",
    "trainDf, testDf = split_to_trainDF_testDF()\n",
    "\n",
    "y_train = trainDf['gender']\n",
    "y_test = testDf['gender']\n",
    "#Create vectorizer\n",
    "vec = create_count_vectorizer(2)\n",
    "\n",
    "# Transform text data into numerical features\n",
    "x_train = vectorizer.fit_transform(trainDf['story'])\n",
    "x_test = vectorizer.transform(testDf['story'])\n",
    "\n",
    "#Create SGDClassifier model without the hyper parameter\n",
    "comparison_model = SGDClassifier()\n",
    "\n",
    "# Train the chosen model\n",
    "fit(comparison_model, x_train, y_train)\n",
    "\n",
    "# Predict gender using the trained model\n",
    "y_pred_comparison_model = predict(comparison_model, x_test)\n",
    "\n",
    "#Calc f1_score and accuracy of the model\n",
    "comparison_model_accuracy = evaluate_accuracy(y_test, y_pred_comparison_model)\n",
    "comparison_model_f1_score = f1_score_calc(testDf, y_pred_comparison_model)\n",
    "\n",
    "print(f\"Now we can see that the f1_score of the comparison_model decreased to {comparison_model_f1_score} instead {best_model_dict['average_f1_score']}\")\n",
    "print(f\"And the accuracy of the comparison_model decreased to {comparison_model_accuracy} instead {best_model_dict['evaluate_accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save output to csv (optional)\n",
    "After you're done save your output to the 'classification_results.csv' csv file.<br/>\n",
    "We assume that the dataframe with your results contain the following columns:\n",
    "* column 1 (left column): 'test_example_id'  - the same id associated to each of the test stories to be predicted.\n",
    "* column 2 (right column): 'predicted_category' - the predicted gender value for each of the associated story. \n",
    "\n",
    "Assuming your predicted values are in the `df_predicted` dataframe, you should save you're results as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predicted.to_csv('classification_results.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
